# Critical Thinking

The most dangerous errors are the ones you are most confident about.

Not because confidence is always wrong, but because confidence closes the loop. The person who is certain does not look for disconfirming evidence. They do not notice when their reasoning is circular. They mistake the familiarity of an idea for evidence of its truth. Confidence that has not been earned through rigorous examination is not a cognitive achievement — it is a cognitive blind spot wearing one.

Critical thinking is not skepticism about everything. Blanket skepticism is its own form of bad reasoning — it allows you to dismiss anything you find inconvenient by raising doubts rather than engaging with evidence. The cynic and the gullible person share the same deficiency: neither is actually evaluating the strength of the argument in front of them. The cynic deflects it with suspicion; the gullible person accepts it on trust. Critical thinking is the discipline that lives between these failures. It means asking about the quality of the reasoning and the evidence, rather than either accepting or rejecting on the basis of prior disposition.

## The Errors That Have Names

Most bad reasoning is not stupid. It is recognizable, repeated, and has names. Confirmation bias — seeking evidence that supports what you already believe while discounting evidence that challenges it — is the most pervasive. You are not immune to it because you are educated or intelligent. Research consistently shows that higher cognitive ability, unaccompanied by specific critical thinking practice, makes people better at rationalizing the conclusions they already hold, not better at actually reaching accurate ones. The smart person often has a more sophisticated version of the same bias as anyone else.

Ad hominem reasoning attacks the person making an argument rather than the argument itself. This fails because the validity of an argument does not depend on the character of the person making it. A claim made by someone with bad motives can still be true. A claim made by someone you admire can still be false. Evaluating the argument requires separating the argument from its source — something that requires genuine effort because our social cognition is powerfully attuned to source rather than content.

The false dichotomy: presenting two options as if they are the only ones when a wider range of positions exists. The straw man: characterizing an opponent's position in a weakened or distorted form so it is easier to refute. Appeal to authority: treating the opinion of an expert as definitive rather than as evidence to be weighed. These are not exotic errors. They populate political speech, news commentary, and ordinary conversation every day. Recognizing them is a learnable skill, and it changes the experience of encountering arguments — you begin to notice the move before the conclusion arrives.

## Motivated Reasoning

Motivated reasoning deserves particular attention because it is subtle and because everyone does it without knowing. Motivated reasoning is not lying. It is the mind working backward from a desired conclusion to the reasoning that supports it, while presenting the result as if it had moved forward from evidence to conclusion. The person practicing motivated reasoning is usually not aware of it. They feel as though they are reasoning normally. What distinguishes motivated from genuine reasoning is often visible only in asymmetry: the same standards are not applied to evidence on both sides of the question. Claims supporting the desired conclusion pass easily; claims challenging it are subjected to intense scrutiny. If you find that your evidential standards shift depending on which way the result would point, you are reasoning motivated rather than clearly.

## Provisional Conclusions, Not Paralysis

Holding provisional conclusions is not the same as having no conclusions. This is a critical distinction for anyone who takes critical thinking seriously. The goal is not to remain in perpetual undecided suspension, unwilling to commit to anything until certainty is achieved — certainty is almost never achievable. The goal is to hold your conclusions with appropriate confidence: high confidence where evidence and reasoning are strong and convergent, lower confidence where they are weak or contested, and genuine openness to revision when better evidence arrives. The scientist who changes their position when confronted with compelling contrary evidence is not being weak. They are doing exactly what the method requires.

There is a social dimension to this. Critical thinking is often inconvenient because it tends to complicate things that feel simple, and because it occasionally leads to conclusions that are unpopular in your particular community. It requires a degree of independence from group consensus that is cognitively and socially costly. The person who applies critical thinking consistently will occasionally reach conclusions that their peers do not share, and will have to decide whether the reasoning matters more than the belonging. This is not a small ask. Human beings are deeply social, and the pressure to conform one's beliefs to the group is real. Intellectual independence, like other forms of integrity, costs something.

The capacity to change your mind based on evidence is not a weakness. It is one of the few things that distinguishes a thinking person from an algorithm running a fixed program.

The question is not whether you are confident. It is whether you have earned it.
